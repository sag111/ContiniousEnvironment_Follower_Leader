{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e7404c-54de-4de0-b5bb-fc8062ce04c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# если я правильно помню, без такой переменной gym будет пытаться отрендерить изображение на гпу и могут возникнуть проблемы на кластере.\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "os.chdir(\"..\")  # !!!  сменим папке, чтоб сервер ray увидел код continuous_grid_arctic (возможно как-то подругому можно, sys.path не помогает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff5c4b27-8ae7-47a0-a40c-9ce58424705f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.8.0)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "import ray\n",
    "from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
    "#import pygame\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "import continuous_grid_arctic.follow_the_leader_continuous_env  # при импорте происходит регистрация сред\n",
    "from continuous_grid_arctic.utils.wrappers import ContinuousObserveModifier_v0\n",
    "from ray.tune import register_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a4c83c1-7c9c-46a9-ad00-a199dbca0845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuous_env_maker(config):\n",
    "    \"\"\"\n",
    "    Функция создающая среду для робота по переданному конфигу.\n",
    "    В конфиге должны быть поля:\n",
    "     - name - имя среды, под которым она была зарегистрирована через gym_register. # сейчас регистрация происходит при импорте follow_the_leader_continuous_env\n",
    "     - base_env_config - словарь с аргументами, которые будут переданы в конструктор класса gym среды\n",
    "     - wrappers - список названий классов обёрток, которые будут добавлены к среде для обработки наблюдений и экшенов.\n",
    "    \"\"\"\n",
    "    name = config[\"name\"]\n",
    "    action_values_range = config.get(\"action_values_range\", None)\n",
    "    env = gym.make(name, **config[\"base_env_config\"])\n",
    "    if 'ContinuousObserveModifier_v0' in config[\"wrappers\"]:\n",
    "        env = ContinuousObserveModifier_v0(env, action_values_range)    \n",
    "    return env\n",
    "\n",
    "register_env(\"continuous-grid\", continuous_env_maker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c0ead14-cd51-4729-a3f0-45226ef73c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    'name' : 'Test-Cont-Env-Auto-v0',  #  имя среды, под которым она была зарегистрирована через gym_register. # сейчас регистрация происходит при импорте follow_the_leader_continuous_env, посмотреть доступные названия можно в follow_the_leader_continuous_env.py\n",
    "    'wrappers': ['ContinuousObserveModifier_v0'],  # список названий классов обёрток, которые будут добавлены к среде для обработки наблюдений и экшенов.\n",
    "    'base_env_config' : {  # словарь с аргументами, которые будут переданы в конструктор класса gym среды\n",
    "        'add_obstacles': True,  # наличие препятствий в среде\n",
    "        'max_distance': 4,  # максимальная дистанция, на которую может отстать ведомый (метры)\n",
    "        'warm_start': 0,  # количество шагов, в течение которых не начислаются награды и штрафы\n",
    "        'max_steps': 10000,  # количество шагов, после которого остановится симуляция\n",
    "        'framerate': 5000,  # ? максимальная частота шагов?\n",
    "        'obstacle_number': 35,  # количество камней\n",
    "        'constant_follower_speed': False,  # зафиксировать скорость ведомого и оставить только 1 действие - поворот\n",
    "        'corridor_length': 10,  # длина коридора сейф зоны (с точки зрения признака, но не награды)  # !!! вообще это должен быть параметр сенсора\n",
    "        'corridor_width': 1.5,  # ширина коридора сейф зоны (с точки зрения признака, но не награды)  # !!! вообще это должен быть параметр сенсора\n",
    "        'move_bear_v4': True,  #?\n",
    "        'follower_speed_koeff': 0.6,  # коэффициент скорости ведомого\n",
    "        'leader_speed_coeff': 0.45,  # коэффициент скорости лидера\n",
    "        'negative_speed': True,  # можно ли двигаться назад\n",
    "        'add_bear': True,  # добавлять ли медведя (динамическое препатствие)\n",
    "        'bear_number': 1,  # количество динамических препятствий\n",
    "        'bear_speed_coeff': 1.2,  # коэффициент скорости медведя                \n",
    "        'multi_random_bears': False,  # ?\n",
    "        'bear_behind': False,  # ?\n",
    "        'use_prev_obs': True,  # использовать предыдущие наблюдения\n",
    "        'max_prev_obs': 5,  # количество предыдущих наблюдений\n",
    "        'early_stopping' : {  # когда стоит останавливать симуляцию (помимо максимального количества шагов)\n",
    "            'max_distance_coef': 3.5,  # когда ведомый удалится от лидера больше чем на max_distance*max_distance_coef метров\n",
    "            'low_reward': -300  # когда суммарная награда опустится ниже этого значения\n",
    "        },\n",
    "        'random_frames_per_step': [2, 20],  # количество фреймов (изменений происходящих в среде) на один шаг (принятие решения)\n",
    "        'leader_speed_regime': {  # расписание коэффициентов скорости лидера\n",
    "            '0': [0.2, 1],\n",
    "            '200': 1,\n",
    "            '1000': [0.5, 1],\n",
    "            '1500': 0.75,\n",
    "            '2000': 0,\n",
    "            '2500': 1,\n",
    "            '3000': [0.5, 1],\n",
    "            '4000': [0.0, 0.5],\n",
    "            '5000': [0.4, 1]\n",
    "        },\n",
    "        'follower_sensors': {  # активные сенсоры ведомого и их настройки\n",
    "            'LeaderPositionsTracker_v2': {  \n",
    "                'eat_close_points': False,\n",
    "                'generate_corridor': True,\n",
    "                'saving_period': 8,\n",
    "                'sensor_name': 'LeaderPositionsTracker_v2',\n",
    "                'start_corridor_behind_follower': True,\n",
    "            },\n",
    "            'LeaderCorridor_Prev_lasers_v2': {\n",
    "                'sensor_name': 'LeaderCorridor_Prev_lasers_v2',\n",
    "                'react_to_obstacles':  True,\n",
    "                'react_to_safe_corridor': True,\n",
    "                'react_to_green_zone': True,\n",
    "                'front_lasers_count': 6,\n",
    "                'back_lasers_count': 6,\n",
    "                'laser_length': 150\n",
    "            },\n",
    "            'LaserPrevSensor': {\n",
    "                'sensor_name': 'LaserPrevSensor',\n",
    "                'react_to_obstacles': True,\n",
    "                'react_to_safe_corridor': False,\n",
    "                'react_to_green_zone': False,\n",
    "                'front_lasers_count': 15,\n",
    "                'back_lasers_count':15,\n",
    "                'laser_length': 200\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff62f7b-749e-4908-8528-e128d9038d8e",
   "metadata": {},
   "source": [
    "run_config = { \n",
    "    'env': 'continuous-grid',\n",
    "    'run': 'PPO',\n",
    "    'local_dir': 'notebooks/demo_run_logs',\n",
    "    'checkpoint_freq': 20,\n",
    "    'stop': {\n",
    "        'training_iteration': 100,\n",
    "    },\n",
    "    'config': {\n",
    "        'num_gpus': 1,\n",
    "        #'timesteps_per_iteration': 1000,\n",
    "        'num_workers': 1,\n",
    "        'log_level': 'ERROR',\n",
    "        'framework': 'tf',\n",
    "        'env': 'continuous-grid',\n",
    "        'normalize_actions': False,\n",
    "        'env_config': env_config,\n",
    "        'train_batch_size': 10000,\n",
    "        'sgd_minibatch_size': 256,\n",
    "       # 'model': {\n",
    "            #'custom_model': 'def_m_prev',\n",
    "       # }\n",
    "    }\n",
    "}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3c77e29-fab3-4336-9de8-44fe9a59e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# конфигурация запуска рэя\n",
    "# https://docs.ray.io/en/releases-1.11.0/rllib/rllib-training.html#common-parameters\n",
    "# https://docs.ray.io/en/releases-1.11.0/rllib/rllib-algorithms.html#proximal-policy-optimization-ppo\n",
    "run_config = {\n",
    "    'num_gpus': 1,\n",
    "    'num_workers': 1,\n",
    "    'log_level': 'ERROR',\n",
    "    'framework': 'tf',\n",
    "    'env': 'continuous-grid',\n",
    "    'normalize_actions': False,\n",
    "    'env_config': env_config,\n",
    "    'train_batch_size': 1024,  # сколько шагов будет сэмплировано перед тем, как начать обучение. \n",
    "    'sgd_minibatch_size': 256\n",
    "}        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91fe36c0-fdec-4ae5-8d41-5838ecfbfe61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.19.8.129',\n",
       " 'raylet_ip_address': '172.19.8.129',\n",
       " 'redis_address': '172.19.8.129:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2023-06-09_17-27-53_928869_24893/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2023-06-09_17-27-53_928869_24893/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2023-06-09_17-27-53_928869_24893',\n",
       " 'metrics_export_port': 59216,\n",
       " 'node_id': '99de16e6adf264f5747dd86dc417df96d590049d2167756caaceeafc'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d91306b-1611-4d6b-85bf-d6afb4bdbfd0",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ab4826f-03e1-41f8-b133-72f7911a5529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-09 17:27:57,095\tINFO trainer.py:722 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also want to then set `eager_tracing=True` in order to reach similar execution speed as with static-graph mode.\n",
      "2023-06-09 17:27:57,096\tWARNING ppo.py:143 -- `train_batch_size` (1024) cannot be achieved with your other settings (num_workers=1 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1024.\n",
      "2023-06-09 17:27:57,097\tINFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2023-06-09 17:27:57,098\tINFO trainer.py:743 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=25151)\u001b[0m pygame 2.1.2 (SDL 2.0.16, Python 3.8.0)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25151)\u001b[0m Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25151)\u001b[0m ===Запуск симуляции номер 0===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=25151)\u001b[0m /s/ls4/users/grartem/RL_robots/continuous_grid_arctic/continuous_grid_arctic/follow_the_leader_continuous_env.py:331: UserWarning: Одновременно заданы и random_frames_per_step и frames_per_step, будет использоваться random_frames_per_step\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25151)\u001b[0m   warn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25151)\u001b[0m /s/ls4/users/grartem/anaconda3/envs/rl_robots/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25151)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25151)\u001b[0m Failed to create secure directory (/run/user/1556/pulse): No such file or directory\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25151)\u001b[0m ALSA lib confmisc.c:767:(parse_card) cannot find card '0'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25151)\u001b[0m ALSA lib conf.c:4568:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25151)\u001b[0m ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25151)\u001b[0m ALSA lib conf.c:4568:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25151)\u001b[0m ALSA lib confmisc.c:1246:(snd_func_refer) error evaluating name\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25151)\u001b[0m ALSA lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25151)\u001b[0m ALSA lib conf.c:5047:(snd_config_expand) Evaluate error: No such file or directory\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25151)\u001b[0m ALSA lib pcm.c:2565:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "2023-06-09 17:28:12,617\tINFO trainable.py:124 -- Trainable.setup took 15.524 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2023-06-09 17:28:12,621\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/s/ls4/users/grartem/ray_results/PPO_continuous-grid_2023-06-09_17-27-57jopp9fq3/\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "rllib_trainer = PPOTrainer(config=run_config)\n",
    "print(rllib_trainer.logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b64277f-97ea-46f1-baf3-03b3de51b67f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_results = []\n",
    "for iter_i in range(5):\n",
    "    print(\"\\n### ITERATION:\\n\", iter_i)\n",
    "    # запускаем сбор данных + обучение\n",
    "    iter_results = rllib_trainer.train()\n",
    "    # сохраняем чекпоинт\n",
    "    checkpoint_path = rllib_trainer.save(rllib_trainer.logdir)\n",
    "    iter_results.pop(\"config\")\n",
    "    train_results.append(iter_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e79a3328-1068-42d1-a2aa-ecd7f771d63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episode_media</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>policy_reward_min</th>\n",
       "      <th>policy_reward_max</th>\n",
       "      <th>policy_reward_mean</th>\n",
       "      <th>custom_metrics</th>\n",
       "      <th>...</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time_this_iter_s</th>\n",
       "      <th>time_total_s</th>\n",
       "      <th>pid</th>\n",
       "      <th>hostname</th>\n",
       "      <th>node_ip</th>\n",
       "      <th>time_since_restore</th>\n",
       "      <th>timesteps_since_restore</th>\n",
       "      <th>iterations_since_restore</th>\n",
       "      <th>perf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44.7</td>\n",
       "      <td>-5.5</td>\n",
       "      <td>15.557143</td>\n",
       "      <td>142.00000</td>\n",
       "      <td>{}</td>\n",
       "      <td>7</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>...</td>\n",
       "      <td>1686320970</td>\n",
       "      <td>78.085608</td>\n",
       "      <td>78.085608</td>\n",
       "      <td>24893</td>\n",
       "      <td>g001</td>\n",
       "      <td>172.19.8.129</td>\n",
       "      <td>78.085608</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'cpu_util_percent': 3.7017857142857147, 'ram_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81.5</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>19.800000</td>\n",
       "      <td>143.00000</td>\n",
       "      <td>{}</td>\n",
       "      <td>7</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>...</td>\n",
       "      <td>1686321044</td>\n",
       "      <td>73.335085</td>\n",
       "      <td>151.420692</td>\n",
       "      <td>24893</td>\n",
       "      <td>g001</td>\n",
       "      <td>172.19.8.129</td>\n",
       "      <td>151.420692</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>{'cpu_util_percent': 3.697142857142857, 'ram_u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81.5</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>18.386957</td>\n",
       "      <td>128.26087</td>\n",
       "      <td>{}</td>\n",
       "      <td>9</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>...</td>\n",
       "      <td>1686321126</td>\n",
       "      <td>81.775246</td>\n",
       "      <td>233.195938</td>\n",
       "      <td>24893</td>\n",
       "      <td>g001</td>\n",
       "      <td>172.19.8.129</td>\n",
       "      <td>233.195938</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>{'cpu_util_percent': 3.6724137931034484, 'ram_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81.5</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>27.118750</td>\n",
       "      <td>126.62500</td>\n",
       "      <td>{}</td>\n",
       "      <td>9</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>...</td>\n",
       "      <td>1686321205</td>\n",
       "      <td>79.407805</td>\n",
       "      <td>312.603744</td>\n",
       "      <td>24893</td>\n",
       "      <td>g001</td>\n",
       "      <td>172.19.8.129</td>\n",
       "      <td>312.603744</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>{'cpu_util_percent': 3.7166666666666672, 'ram_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81.5</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>25.661905</td>\n",
       "      <td>120.50000</td>\n",
       "      <td>{}</td>\n",
       "      <td>10</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>...</td>\n",
       "      <td>1686321302</td>\n",
       "      <td>96.838297</td>\n",
       "      <td>409.442040</td>\n",
       "      <td>24893</td>\n",
       "      <td>g001</td>\n",
       "      <td>172.19.8.129</td>\n",
       "      <td>409.442040</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>{'cpu_util_percent': 3.6978260869565216, 'ram_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0                44.7                -5.5            15.557143   \n",
       "1                81.5               -14.0            19.800000   \n",
       "2                81.5               -14.0            18.386957   \n",
       "3                81.5               -14.0            27.118750   \n",
       "4                81.5               -14.0            25.661905   \n",
       "\n",
       "   episode_len_mean episode_media  episodes_this_iter policy_reward_min  \\\n",
       "0         142.00000            {}                   7                {}   \n",
       "1         143.00000            {}                   7                {}   \n",
       "2         128.26087            {}                   9                {}   \n",
       "3         126.62500            {}                   9                {}   \n",
       "4         120.50000            {}                  10                {}   \n",
       "\n",
       "  policy_reward_max policy_reward_mean custom_metrics  ...   timestamp  \\\n",
       "0                {}                 {}             {}  ...  1686320970   \n",
       "1                {}                 {}             {}  ...  1686321044   \n",
       "2                {}                 {}             {}  ...  1686321126   \n",
       "3                {}                 {}             {}  ...  1686321205   \n",
       "4                {}                 {}             {}  ...  1686321302   \n",
       "\n",
       "  time_this_iter_s time_total_s    pid  hostname       node_ip  \\\n",
       "0        78.085608    78.085608  24893      g001  172.19.8.129   \n",
       "1        73.335085   151.420692  24893      g001  172.19.8.129   \n",
       "2        81.775246   233.195938  24893      g001  172.19.8.129   \n",
       "3        79.407805   312.603744  24893      g001  172.19.8.129   \n",
       "4        96.838297   409.442040  24893      g001  172.19.8.129   \n",
       "\n",
       "   time_since_restore timesteps_since_restore iterations_since_restore  \\\n",
       "0           78.085608                       0                        1   \n",
       "1          151.420692                       0                        2   \n",
       "2          233.195938                       0                        3   \n",
       "3          312.603744                       0                        4   \n",
       "4          409.442040                       0                        5   \n",
       "\n",
       "                                                perf  \n",
       "0  {'cpu_util_percent': 3.7017857142857147, 'ram_...  \n",
       "1  {'cpu_util_percent': 3.697142857142857, 'ram_u...  \n",
       "2  {'cpu_util_percent': 3.6724137931034484, 'ram_...  \n",
       "3  {'cpu_util_percent': 3.7166666666666672, 'ram_...  \n",
       "4  {'cpu_util_percent': 3.6978260869565216, 'ram_...  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas  as pd\n",
    "pd.DataFrame(train_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138f18f9-dbbd-41d9-965f-30b6ee91ffef",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12cacf07-ea75-4042-92bd-a5a209b3fa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-09 17:35:03,332\tINFO trainable.py:467 -- Restored on 172.19.8.129 from checkpoint: /s/ls4/users/grartem/ray_results/PPO_continuous-grid_2023-06-09_17-27-57jopp9fq3/checkpoint_000005/checkpoint-5\n",
      "2023-06-09 17:35:03,334\tINFO trainable.py:475 -- Current state after restoring: {'_iteration': 5, '_timesteps_total': 0, '_time_total': 409.44204020500183, '_episodes_total': 42}\n"
     ]
    }
   ],
   "source": [
    "# Загружаем последний чекпоинт\n",
    "rllib_trainer.restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec7c1973-ab35-43bc-aac8-3e0b1e60a545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Запуск симуляции номер 2===\n",
      "АВАРИЯ! -10.0\n",
      "АВАРИЯ! -10.0\n",
      "АВАРИЯ! -10.0\n",
      "АВАРИЯ! -10.0\n",
      "АВАРИЯ! -10.0\n",
      "АВАРИЯ! -10.0\n",
      "АВАРИЯ! -10.0\n",
      "АВАРИЯ! -10.0\n",
      "АВАРИЯ! -10.0\n",
      "АВАРИЯ! -10.0\n",
      "АВАРИЯ! -10.0\n",
      "АВАРИЯ! -10.0\n",
      "АВАРИЯ! -10.0\n",
      "===Запуск симуляции номер 3===\n"
     ]
    }
   ],
   "source": [
    "# создаём экземпляр среды для теста (возомжно его как-то из тренера можно достать)\n",
    "env = continuous_env_maker(config=env_config)\n",
    "# добавляем обёртку из gym для сохранения видео\n",
    "monitor_env = gym.wrappers.Monitor(env=env,\n",
    "                                    directory= os.path.dirname(checkpoint_path)+\"/videos\",\n",
    "                                   video_callable=lambda _:True,\n",
    "                                   force=False,\n",
    "                                  uid=\"video_test\", mode=\"evaluation\")\n",
    "\n",
    "monitor_env.seed(12)  # выбираем какой-нибудь рандом сид (фиксируем расположение объектов)\n",
    "obs = monitor_env.reset()  # сбрасываем среду  (сид применяется как раз здесь), получаем первое наблюдение\n",
    "done = False\n",
    "while not done:\n",
    "    # определяем действие по наблюдению\n",
    "    action = rllib_trainer.compute_single_action(obs, explore=False)\n",
    "    # применяем действие, получаем следующее наблюдение и награду\n",
    "    obs, reward, done, info = monitor_env.step(action)\n",
    "obs = monitor_env.reset()  # чтобы завершить видео файл, надо вызвать сброс. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_robots",
   "language": "python",
   "name": "rl_robots"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
